<div>
<style>
/* AI ê¸°ìˆ  ë¸”ë¡œê·¸ í‘œì¤€ ìŠ¤íƒ€ì¼ */
body {
  font-family: "Noto Sans KR", "Segoe UI", sans-serif;
  font-size: 17px;
  line-height: 1.8;
  color: #333;
}
p {
    margin-bottom: 1.2em;
    word-break: keep-all;
}
h2 {
  border-bottom: 2px solid #0066cc;
  padding-bottom: 10px;
  margin-top: 40px;
  margin-bottom: 20px;
  font-weight: bold;
  color: #0066cc;
}
h3 {
    font-size: 1.2em;
    margin-top: 30px;
    margin-bottom: 15px;
    border-bottom: 1px solid #e6f2ff;
    padding-bottom: 5px;
    color: #0066cc;
}
.concept-block {
    background-color: #f0f8ff;
    border-left: 4px solid #0066cc;
    padding: 15px 20px;
    margin: 20px 0;
    border-radius: 0 8px 8px 0;
}
.code-block {
    background-color: #f5f5f5;
    border: 1px solid #ddd;
    border-radius: 8px;
    padding: 15px;
    margin: 20px 0;
    font-family: 'Courier New', monospace;
}
.performance-table {
  width: 100%;
  border-collapse: collapse;
  margin: 20px 0;
  font-size: 16px;
  text-align: center;
}
.performance-table th, .performance-table td {
  border: 1px solid #ddd;
  padding: 12px;
}
.performance-table th {
  background-color: #0066cc;
  color: white;
  font-weight: bold;
}
.performance-table tr:nth-child(even) {
  background-color: #f2f2f2;
}
.ai-tech {
  background-color: #e6f3ff;
  padding: 3px 8px;
  border-radius: 4px;
  font-weight: bold;
  color: #0066cc;
}
.warning {
    background-color: #fff3cd;
    border-left: 5px solid #ffc107;
    padding: 15px;
    margin: 20px 0;
}
.info {
    background-color: #d1ecf1;
    border-left: 5px solid #17a2b8;
    padding: 15px;
    margin: 20px 0;
}
</style>
</div>
<h2 id="introduction" data-ke-size="size26">ğŸ¤– Omnilingual ASR: ë‹¤êµ­ì–´ ìŒì„± ì¸ì‹ì˜ ìƒˆë¡œìš´ íŒ¨ëŸ¬ë‹¤ì„</h2>
<p data-ke-size="size16"><span class="ai-tech">AI ê¸°ìˆ </span>ì´ ë°œì „í•˜ë©´ì„œ ìŒì„± ì¸ì‹(Automatic Speech Recognition, ASR) ë¶„ì•¼ì—ì„œë„ ë†€ë¼ìš´ ë³€í™”ê°€ ì¼ì–´ë‚˜ê³  ìˆìŠµë‹ˆë‹¤. íŠ¹íˆ Facebook Researchì—ì„œ ê°œë°œí•œ <b>Omnilingual ASR</b>ì€ <b>100ê°œ ì´ìƒì˜ ì–¸ì–´</b>ë¥¼ ì§€ì›í•˜ëŠ” í˜ì‹ ì ì¸ ë‹¤êµ­ì–´ ìŒì„± ì¸ì‹ ëª¨ë¸ë¡œ, ì „ ì„¸ê³„ ì‚¬ìš©ìë“¤ì—ê²Œ ì–¸ì–´ ì¥ë²½ ì—†ëŠ” ìŒì„± ì¸ì‹ ê²½í—˜ì„ ì œê³µí•˜ê³  ìˆìŠµë‹ˆë‹¤.</p>
<p data-ke-size="size16">ì´ë²ˆ í¬ìŠ¤íŒ…ì—ì„œëŠ” Omnilingual ASRì˜ ê¸°ìˆ ì  íŠ¹ì§•ê³¼ í•µì‹¬ ì›ë¦¬ë¥¼ ì‹¬ì¸µì ìœ¼ë¡œ ë¶„ì„í•˜ê³ , ì´ˆë³´ìë„ ì‰½ê²Œ ë”°ë¼í•  ìˆ˜ ìˆëŠ” ì‹¤ì „ ì˜ˆì œë¥¼ í†µí•´ ì‹¤ì œ êµ¬í˜„ ë°©ë²•ì„ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤.</p>
<h2 id="technical-overview" data-ke-size="size26">ğŸ” Omnilingual ASR ê¸°ìˆ ì  íŠ¹ì§•</h2>
<div class="concept-block">
<h3 data-ke-size="size23">í•µì‹¬ ì•„í‚¤í…ì²˜</h3>
<p data-ke-size="size16">Omnilingual ASRì€ <b>Transformer ê¸°ë°˜ì˜ ì¸ì½”ë”-ë””ì½”ë” ì•„í‚¤í…ì²˜</b>ë¥¼ ì±„íƒí•˜ê³  ìˆìŠµë‹ˆë‹¤. íŠ¹íˆ ë‹¤ìŒê³¼ ê°™ì€ í˜ì‹ ì ì¸ ê¸°ìˆ ë“¤ì´ ì ìš©ë˜ì—ˆìŠµë‹ˆë‹¤:</p>
<ul style="list-style-type: disc;" data-ke-list-type="disc">
<li><b>ì–¸ì–´ ë…ë¦½ì ì¸ ìŒì„± í‘œí˜„</b>: ì–¸ì–´ì— ê´€ê³„ì—†ì´ ìŒì„±ì˜ ê³µí†µì ì¸ íŠ¹ì§•ì„ ì¶”ì¶œí•˜ëŠ” ì¸ì½”ë”</li>
<li><b>ë‹¤êµ­ì–´ í† í¬ë‚˜ì´ì €</b>: 100ê°œ ì–¸ì–´ë¥¼ ë‹¨ì¼ í† í¬ë‚˜ì´ì €ë¡œ ì²˜ë¦¬í•˜ëŠ” ìœ ë‹ˆë²„ì„¤ í† í¬ë‚˜ì´ì €</li>
<li><b>ì „ì´ í•™ìŠµ(Transfer Learning)</b>: ê³ ìì› ì–¸ì–´ì—ì„œ í•™ìŠµëœ ì§€ì‹ì„ ì €ìì› ì–¸ì–´ë¡œ ì „ì´</li>
</ul>
</div>
<h3 data-ke-size="size23">ì„±ëŠ¥ ë¹„êµ</h3>
<div style="overflow-x: auto;">
<table class="performance-table" data-ke-align="alignLeft">
<thead>
<tr>
<th>ì–¸ì–´ ê·¸ë£¹</th>
<th>ì§€ì› ì–¸ì–´ ìˆ˜</th>
<th>WER(%)</th>
<th>íŠ¹ì§•</th>
</tr>
</thead>
<tbody>
<tr>
<td>ê³ ìì› ì–¸ì–´</td>
<td>15ê°œ</td>
<td>8.2</td>
<td>ì˜ì–´, ì¤‘êµ­ì–´, ìŠ¤í˜ì¸ì–´ ë“±</td>
</tr>
<tr>
<td>ì¤‘ê°„ ìì› ì–¸ì–´</td>
<td>35ê°œ</td>
<td>12.5</td>
<td>í•œêµ­ì–´, ì¼ë³¸ì–´, ë…ì¼ì–´ ë“±</td>
</tr>
<tr>
<td>ì €ìì› ì–¸ì–´</td>
<td>50ê°œ</td>
<td>18.7</td>
<td>ì•„í”„ë¦¬ì¹´, ë™ë‚¨ì•„ì‹œì•„ ì–¸ì–´ ë“±</td>
</tr>
</tbody>
</table>
</div>
<div class="info">
<h3 data-ke-size="size23">ğŸ’¡ ê¸°ìˆ ì  í†µì°°</h3>
<p data-ke-size="size16">Omnilingual ASRì˜ ê°€ì¥ í° ì¥ì ì€ <b>ë‹¨ì¼ ëª¨ë¸ë¡œ ë‹¤êµ­ì–´ë¥¼ ì²˜ë¦¬</b>í•œë‹¤ëŠ” ì ì…ë‹ˆë‹¤. ê¸°ì¡´ì—ëŠ” ê° ì–¸ì–´ë³„ë¡œ ë³„ë„ì˜ ëª¨ë¸ì„ í•™ìŠµí•´ì•¼ í–ˆì§€ë§Œ, ì´ì œëŠ” í•˜ë‚˜ì˜ ëª¨ë¸ë¡œ 100ê°œ ì–¸ì–´ë¥¼ ëª¨ë‘ ì²˜ë¦¬í•  ìˆ˜ ìˆê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤.</p>
</div>
<h2 id="how-it-works" data-ke-size="size26">âš™ï¸ ì‘ë™ ì›ë¦¬ ì‹¬ì¸µ ë¶„ì„</h2>
<p data-ke-size="size16">Omnilingual ASRì˜ í•µì‹¬ ì‘ë™ ì›ë¦¬ë¥¼ ë‹¨ê³„ë³„ë¡œ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.</p>
<h3 data-ke-size="size23">1. ìŒì„± ì „ì²˜ë¦¬ ë‹¨ê³„</h3>
<div class="concept-block">
<p data-ke-size="size16">ìŒì„± ì‹ í˜¸ëŠ” ë¨¼ì € <b>ë©œ ìŠ¤í™íŠ¸ë¡œê·¸ë¨(Mel Spectrogram)</b>ìœ¼ë¡œ ë³€í™˜ë©ë‹ˆë‹¤. ì´ ê³¼ì •ì—ì„œ ì¸ê°„ì˜ ì²­ê° íŠ¹ì„±ì„ ëª¨ë°©í•œ ë©œ ìŠ¤ì¼€ì¼ì„ ì ìš©í•˜ì—¬ ìŒì„±ì˜ ì£¼ìš” íŠ¹ì§•ì„ íš¨ê³¼ì ìœ¼ë¡œ ì¶”ì¶œí•©ë‹ˆë‹¤.</p>
</div>
<h3 data-ke-size="size23">2. ì¸ì½”ë”: ìŒì„± íŠ¹ì§• ì¶”ì¶œ</h3>
<p data-ke-size="size16">Transformer ì¸ì½”ë”ëŠ” ìŒì„± ì‹œí€€ìŠ¤ì—ì„œ <b>ì–¸ì–´ ë¶ˆë³€ì ì¸ ìŒì„± í‘œí˜„</b>ì„ í•™ìŠµí•©ë‹ˆë‹¤. ì´ë•Œ <b>ì…€í”„ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜</b>ì„ ì‚¬ìš©í•˜ì—¬ ìŒì„±ì˜ ì¥ê±°ë¦¬ ì˜ì¡´ì„±ì„ íš¨ê³¼ì ìœ¼ë¡œ ëª¨ë¸ë§í•©ë‹ˆë‹¤.</p>
<h3 data-ke-size="size23">3. ë””ì½”ë”: í…ìŠ¤íŠ¸ ìƒì„±</h3>
<p data-ke-size="size16">ë””ì½”ë”ëŠ” ì¸ì½”ë”ê°€ ì¶”ì¶œí•œ ìŒì„± í‘œí˜„ì„ ê¸°ë°˜ìœ¼ë¡œ <b>ìë™ íšŒê·€ ë°©ì‹</b>ìœ¼ë¡œ í…ìŠ¤íŠ¸ ì‹œí€€ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ì´ë•Œ ë‹¤êµ­ì–´ í† í¬ë‚˜ì´ì €ë¥¼ í™œìš©í•˜ì—¬ ì–¸ì–´ì— ë§ëŠ” í…ìŠ¤íŠ¸ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.</p>
<h2 id="getting-started" data-ke-size="size26">ğŸš€ ì‹¤ì „ íŠœí† ë¦¬ì–¼: Omnilingual ASR ì‹œì‘í•˜ê¸°</h2>
<p data-ke-size="size16">ì´ì œ ì´ˆë³´ìë„ ì‰½ê²Œ ë”°ë¼í•  ìˆ˜ ìˆëŠ” ì‹¤ì „ ì˜ˆì œë¥¼ í†µí•´ Omnilingual ASRì„ ì§ì ‘ êµ¬í˜„í•´ë³´ê² ìŠµë‹ˆë‹¤.</p>
<div class="warning">
<h3 data-ke-size="size23">âš ï¸ ì‚¬ì „ ì¤€ë¹„ì‚¬í•­</h3>
<p data-ke-size="size16">ì‹œì‘í•˜ê¸° ì „ì— ë‹¤ìŒê³¼ ê°™ì€ í™˜ê²½ì´ ì¤€ë¹„ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤:</p>
<ul style="list-style-type: disc;" data-ke-list-type="disc">
<li>Python 3.8 ì´ìƒ</li>
<li>PyTorch 1.9 ì´ìƒ</li>
<li>ìµœì†Œ 8GB RAM (ê¶Œì¥: 16GB)</li>
<li>GPU í™˜ê²½ (ê¶Œì¥: RTX 3060 ì´ìƒ)</li>
</ul>
</div>
<h3 data-ke-size="size23">1ë‹¨ê³„: í™˜ê²½ ì„¤ì •</h3>
<div class="code-block">
<pre class="cmake"><code># ê°€ìƒí™˜ê²½ ìƒì„±
python -m venv omnilingual_env
source omnilingual_env/bin/activate  # Linux/Mac
# omnilingual_env\Scripts\activate  # Windows

# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
pip install torch torchvision torchaudio
pip install transformers datasets
pip install librosa soundfile
pip install jiwer  # WER ê³„ì‚°ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
</code></pre>
</div>
<h3 data-ke-size="size23">2ë‹¨ê³„: ëª¨ë¸ ë¡œë“œ ë° ì„¤ì •</h3>
<div class="code-block">
<pre class="haskell"><code>import torch
import librosa
from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor
import soundfile as sf
import numpy as np

# ëª¨ë¸ê³¼ í”„ë¡œì„¸ì„œ ë¡œë“œ
model_name = "facebook/omniASR_CTC_1B"
processor = Wav2Vec2Processor.from_pretrained(model_name)
model = Wav2Vec2ForCTC.from_pretrained(model_name)

# GPU ì‚¬ìš© ì„¤ì •
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)

print(f"ëª¨ë¸ì´ {device}ì— ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
print(f"ì§€ì› ì–¸ì–´ ìˆ˜: {processor.tokenizer.vocab_size}")
</code></pre>
</div>
<h3 data-ke-size="size23">3ë‹¨ê³„: ìŒì„± íŒŒì¼ ì „ì²˜ë¦¬ í•¨ìˆ˜</h3>
<div class="code-block">
<pre class="python"><code>def preprocess_audio(audio_path, target_sr=16000):
    """
    ìŒì„± íŒŒì¼ì„ ì „ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜
    """
    # ìŒì„± íŒŒì¼ ë¡œë“œ
    speech, sr = librosa.load(audio_path, sr=target_sr)
    
    # ì •ê·œí™”
    speech = speech / np.max(np.abs(speech))
    
    return speech

def transcribe_audio(audio_path, language="korean"):
    """
    ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
    """
    # ìŒì„± ì „ì²˜ë¦¬
    speech = preprocess_audio(audio_path)
    
    # ì…ë ¥ ê°’ìœ¼ë¡œ ë³€í™˜
    inputs = processor(speech, sampling_rate=16000, return_tensors="pt", padding=True)
    inputs = inputs.to(device)
    
    # ëª¨ë¸ ì¶”ë¡ 
    with torch.no_grad():
        logits = model(**inputs).logits
    
    # í…ìŠ¤íŠ¸ë¡œ ë””ì½”ë”©
    predicted_ids = torch.argmax(logits, dim=-1)
    transcription = processor.batch_decode(predicted_ids)
    
    return transcription[0]

# ì‚¬ìš© ì˜ˆì‹œ
audio_file = "sample_audio.wav"  # ìŒì„± íŒŒì¼ ê²½ë¡œ
result = transcribe_audio(audio_file)
print(f"ì¸ì‹ ê²°ê³¼: {result}")
</code></pre>
</div>
<h3 data-ke-size="size23">4ë‹¨ê³„: ë°°ì¹˜ ì²˜ë¦¬ ë° ì„±ëŠ¥ ìµœì í™”</h3>
<div class="code-block">
<pre class="python"><code>def batch_transcribe(audio_files, batch_size=4):
    """
    ì—¬ëŸ¬ ìŒì„± íŒŒì¼ì„ ë°°ì¹˜ë¡œ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜
    """
    results = []
    
    for i in range(0, len(audio_files), batch_size):
        batch_files = audio_files[i:i+batch_size]
        batch_speeches = []
        
        # ë°°ì¹˜ ë‚´ ìŒì„± íŒŒì¼ ì „ì²˜ë¦¬
        for audio_file in batch_files:
            speech = preprocess_audio(audio_file)
            batch_speeches.append(speech)
        
        # ë°°ì¹˜ ì…ë ¥ìœ¼ë¡œ ë³€í™˜
        inputs = processor(batch_speeches, sampling_rate=16000, return_tensors="pt", padding=True)
        inputs = inputs.to(device)
        
        # ë°°ì¹˜ ì¶”ë¡ 
        with torch.no_grad():
            logits = model(**inputs).logits
        
        # ë°°ì¹˜ ë””ì½”ë”©
        predicted_ids = torch.argmax(logits, dim=-1)
        batch_transcriptions = processor.batch_decode(predicted_ids)
        
        results.extend(batch_transcriptions)
        
        print(f"ë°°ì¹˜ {i//batch_size + 1}/{(len(audio_files)-1)//batch_size + 1} ì™„ë£Œ")
    
    return results

# ì„±ëŠ¥ ì¸¡ì •
import time

start_time = time.time()
audio_files = ["audio1.wav", "audio2.wav", "audio3.wav"]  # ì—¬ëŸ¬ ìŒì„± íŒŒì¼
transcriptions = batch_transcribe(audio_files)
end_time = time.time()

print(f"ì´ ì²˜ë¦¬ ì‹œê°„: {end_time - start_time:.2f}ì´ˆ")
print(f"í‰ê·  ì²˜ë¦¬ ì†ë„: {len(audio_files)/(end_time - start_time):.2f} íŒŒì¼/ì´ˆ")
</code></pre>
</div>
<h3 data-ke-size="size23">5ë‹¨ê³„: ê²°ê³¼ í‰ê°€ ë° ìµœì í™”</h3>
<div class="code-block">
<pre class="vim"><code>from jiwer import wer

def evaluate_performance(transcriptions, ground_truths):
    """
    ìŒì„± ì¸ì‹ ì„±ëŠ¥ì„ í‰ê°€í•˜ëŠ” í•¨ìˆ˜
    """
    total_wer = 0
    
    for pred, truth in zip(transcriptions, ground_truths):
        current_wer = wer(truth, pred)
        total_wer += current_wer
        print(f"ì˜ˆì¸¡: {pred}")
        print(f&gt;ì •ë‹µ: {truth}")
        print(f"WER: {current_wer:.4f}")
        print("-" * 50)
    
    avg_wer = total_wer / len(transcriptions)
    print(f"í‰ê·  WER: {avg_wer:.4f}")
    return avg_wer

# ì˜ˆì œ í‰ê°€
ground_truths = ["ì•ˆë…•í•˜ì„¸ìš” ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì¢‹ë„¤ìš”", "ìŒì„± ì¸ì‹ ê¸°ìˆ ì´ ë°œì „í–ˆì–´ìš”"]
transcriptions = ["ì•ˆë…•í•˜ì„¸ìš” ì˜¤ëŠ˜ ë‚ ì‹œê°€ ì¢‹ë„¤ìš”", "ìŒì„± ì¸ì‹ ê¸°ìˆ ì´ ë°œì „í–ˆì–´ìš”"]

evaluate_performance(transcriptions, ground_truths)
</code></pre>
</div>
<h2 id="advanced-techniques" data-ke-size="size26">ğŸ¯ ê³ ê¸‰ í™œìš© ê¸°ë²•</h2>
<p data-ke-size="size16">ê¸°ë³¸ì ì¸ ì‚¬ìš©ë²•ì„ ìµí˜”ë‹¤ë©´, ì´ì œ ë” ê³ ê¸‰ ê¸°ë²•ë“¤ì„ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤.</p>
<h3 data-ke-size="size23">ì–¸ì–´ ì‹ë³„ ë° ìë™ ì „í™˜</h3>
<div class="concept-block">
<p data-ke-size="size16">Omnilingual ASRì€ <b>ì–¸ì–´ ìë™ ì‹ë³„(Language Identification)</b> ê¸°ëŠ¥ì„ ë‚´ì¥í•˜ê³  ìˆìŠµë‹ˆë‹¤. ë‹¤êµ­ì–´ ìŒì„± ë°ì´í„°ê°€ ì£¼ì–´ì§€ë©´ ì–´ë–¤ ì–¸ì–´ì¸ì§€ ìë™ìœ¼ë¡œ íŒë‹¨í•˜ê³  í•´ë‹¹ ì–¸ì–´ì— ë§ëŠ” í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.</p>
</div>
<div class="code-block">
<pre class="ruby"><code>def multilingual_transcribe(audio_path):
    """
    ë‹¤êµ­ì–´ ìŒì„±ì„ ìë™ìœ¼ë¡œ ì‹ë³„í•˜ì—¬ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
    """
    speech = preprocess_audio(audio_path)
    
    # ì–¸ì–´ ì‹ë³„ì„ ìœ„í•œ ì¶”ê°€ ì²˜ë¦¬
    inputs = processor(speech, sampling_rate=16000, return_tensors="pt", padding=True)
    inputs = inputs.to(device)
    
    with torch.no_grad():
        logits = model(**inputs).logits
        predicted_ids = torch.argmax(logits, dim=-1)
    
    # ì–¸ì–´ íƒì§€ ë° ë³€í™˜
    transcription = processor.batch_decode(predicted_ids)
    
    return transcription[0]

# ì‹¤ì‹œê°„ ìŒì„± ì¸ì‹ ìŠ¤íŠ¸ë¦¬ë° ì˜ˆì œ
class StreamingASR:
    def __init__(self, chunk_size=16000):
        self.chunk_size = chunk_size
        self.buffer = []
        
    def process_chunk(self, audio_chunk):
        self.buffer.extend(audio_chunk)
        
        if len(self.buffer) &gt;= self.chunk_size:
            # ì²˜ë¦¬í•  ì²­í¬ ì¶”ì¶œ
            process_chunk = self.buffer[:self.chunk_size]
            self.buffer = self.buffer[self.chunk_size:]
            
            # ìŒì„± ì¸ì‹
            result = multilingual_transcribe_from_chunk(process_chunk)
            return result
        
        return None
</code></pre>
</div>
<h3 data-ke-size="size23">ì„±ëŠ¥ ìµœì í™” íŒ</h3>
<div class="info">
<h3 data-ke-size="size23">âš¡ ìµœì í™” ì „ëµ</h3>
<ul style="list-style-type: disc;" data-ke-list-type="disc">
<li><b>ë°°ì¹˜ í¬ê¸° ìµœì í™”</b>: GPU ë©”ëª¨ë¦¬ì— ë§ëŠ” ìµœì ì˜ ë°°ì¹˜ í¬ê¸°ë¥¼ ì°¾ì•„ë³´ì„¸ìš”</li>
<li><b>ëª¨ë¸ ì–‘ìí™”</b>: ëª¨ë¸ í¬ê¸°ë¥¼ ì¤„ì´ê³  ì¶”ë¡  ì†ë„ë¥¼ ë†’ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤</li>
<li><b>ìºì‹± í™œìš©</b>: ë°˜ë³µì ì¸ ê³„ì‚° ê²°ê³¼ë¥¼ ìºì‹œí•˜ì—¬ ì†ë„ë¥¼ í–¥ìƒì‹œí‚¤ì„¸ìš”</li>
<li><b>ë³‘ë ¬ ì²˜ë¦¬</b>: ë©€í‹°í”„ë¡œì„¸ì‹±ì„ í™œìš©í•˜ì—¬ ì—¬ëŸ¬ ìŒì„± íŒŒì¼ì„ ë™ì‹œì— ì²˜ë¦¬í•˜ì„¸ìš”</li>
</ul>
</div>
<h2 id="real-world-applications" data-ke-size="size26">ğŸŒ ì‹¤ì œ í™œìš© ì‚¬ë¡€</h2>
<p data-ke-size="size16">Omnilingual ASRì€ ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ ì‹¤ì œë¡œ í™œìš©ë˜ê³  ìˆìŠµë‹ˆë‹¤.</p>
<h3 data-ke-size="size23">1. ê¸€ë¡œë²Œ ê³ ê° ì„œë¹„ìŠ¤</h3>
<p data-ke-size="size16">ë‹¤êµ­ì–´ ì½œì„¼í„°ì—ì„œ ê³ ê°ì˜ ì–¸ì–´ë¥¼ ìë™ìœ¼ë¡œ ì‹ë³„í•˜ê³  ì‘ë‹µí•˜ëŠ” ì‹œìŠ¤í…œì— í™œìš©ë©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, í•­ê³µì‚¬ë‚˜ í˜¸í…” ì˜ˆì•½ ì‹œìŠ¤í…œì—ì„œ ì „ ì„¸ê³„ ê³ ê°ë“¤ì˜ ë¬¸ì˜ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>
<h3 data-ke-size="size23">2. êµìœ¡ ë¶„ì•¼</h3>
<p data-ke-size="size16">ì–¸ì–´ í•™ìŠµ ì•±ì—ì„œ ì‚¬ìš©ìì˜ ë°œìŒì„ í‰ê°€í•˜ê³  í”¼ë“œë°±ì„ ì œê³µí•˜ëŠ” ê¸°ëŠ¥ì— í™œìš©ë©ë‹ˆë‹¤. ë‹¤êµ­ì–´ í™˜ê²½ì—ì„œì˜ ì˜¨ë¼ì¸ êµìœ¡ í”Œë«í¼ì—ì„œ ê°•ì˜ ë‚´ìš©ì„ ìë™ìœ¼ë¡œ ìë§‰ ì²˜ë¦¬í•˜ëŠ” ë°ë„ ì‚¬ìš©ë©ë‹ˆë‹¤.</p>
<h3 data-ke-size="size23">3. ì½˜í…ì¸  ì œì‘</h3>
<p data-ke-size="size16">YouTubeë‚˜ íŒŸìºìŠ¤íŠ¸ì™€ ê°™ì€ ë©€í‹°ë¯¸ë””ì–´ ì½˜í…ì¸ ì˜ ìë™ ìë§‰ ìƒì„±ì— í™œìš©ë©ë‹ˆë‹¤. íŠ¹íˆ ë‹¤êµ­ì–´ ì½˜í…ì¸ ì˜ ê²½ìš°, í•œ ë²ˆì— ì—¬ëŸ¬ ì–¸ì–´ì˜ ìë§‰ì„ ìƒì„±í•  ìˆ˜ ìˆì–´ ë§¤ìš° íš¨ìœ¨ì ì…ë‹ˆë‹¤.</p>
<h2 id="future-outlook" data-ke-size="size26">ğŸ”® ë¯¸ë˜ ì „ë§</h2>
<p data-ke-size="size16">Omnilingual ASR ê¸°ìˆ ì€ ì•ìœ¼ë¡œ ë”ìš± ë°œì „í•  ê²ƒì…ë‹ˆë‹¤. <span class="ai-tech">AI ê¸°ìˆ </span>ì˜ ë°œì „ê³¼ í•¨ê»˜ ë‹¤ìŒê³¼ ê°™ì€ ë³€í™”ê°€ ì˜ˆìƒë©ë‹ˆë‹¤:</p>
<ul style="list-style-type: disc;" data-ke-list-type="disc">
<li><b>ë” ë§ì€ ì–¸ì–´ ì§€ì›</b>: í˜„ì¬ 100ê°œ ì–¸ì–´ì—ì„œ 200ê°œ ì´ìƒì˜ ì–¸ì–´ë¡œ í™•ì¥</li>
<li><b>ì‹¤ì‹œê°„ ë²ˆì—­ í†µí•©</b>: ìŒì„± ì¸ì‹ê³¼ ì‹¤ì‹œê°„ ë²ˆì—­ì´ ê²°í•©ëœ ì™„ë²½í•œ ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ ì†”ë£¨ì…˜</li>
<li><b>ê°œì¸í™”ëœ ëª¨ë¸</b>: ì‚¬ìš©ìë³„ ìŒì„± íŠ¹ì„±ì— ë§ì¶° ìµœì í™”ëœ í¼ìŠ¤ë„ ASR</li>
<li><b>ì—£ì§€ ë””ë°”ì´ìŠ¤ ìµœì í™”</b>: ìŠ¤ë§ˆíŠ¸í°ì´ë‚˜ IoT ê¸°ê¸°ì—ì„œ ë¡œì»¬ë¡œ ë™ì‘í•˜ëŠ” ê²½ëŸ‰í™” ëª¨ë¸</li>
</ul>
<h2 id="conclusion" data-ke-size="size26">ğŸ¯ ê²°ë¡ </h2>
<p data-ke-size="size16">Facebook Researchì˜ Omnilingual ASRì€ <b>ë‹¤êµ­ì–´ ìŒì„± ì¸ì‹ ë¶„ì•¼ì˜ í˜ì‹ ì ì¸ breakthrough</b>ì…ë‹ˆë‹¤. ë‹¨ì¼ ëª¨ë¸ë¡œ 100ê°œ ì´ìƒì˜ ì–¸ì–´ë¥¼ ì²˜ë¦¬í•˜ëŠ” ê¸°ìˆ ì  ì„±ì·¨ëŠ” ì „ ì„¸ê³„ì ì¸ ì–¸ì–´ ì¥ë²½ì„ í—ˆë¬´ëŠ” ë° í° ê¸°ì—¬ë¥¼ í•˜ê³  ìˆìŠµë‹ˆë‹¤.</p>
<p data-ke-size="size16">ì˜¤ëŠ˜ ë°°ìš´ ì‹¤ì „ ì˜ˆì œë¥¼ í†µí•´ ì§ì ‘ êµ¬í˜„í•´ë³´ë©´ì„œ <span class="ai-tech">ë¨¸ì‹ ëŸ¬ë‹</span>ê³¼ <span class="ai-tech">ë”¥ëŸ¬ë‹</span>ì˜ ê°•ë ¥í•¨ì„ ì²´ê°í•˜ì…¨ì„ ê²ƒì…ë‹ˆë‹¤. ì´ ê¸°ìˆ ì„ í™œìš©í•˜ì—¬ ìì‹ ë§Œì˜ í˜ì‹ ì ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ê°œë°œí•´ë³´ì‹œëŠ” ê²ƒì„ ì¶”ì²œí•©ë‹ˆë‹¤.</p>
<p data-ke-size="size16">ì—¬ëŸ¬ë¶„ë„ í•œë²ˆ Facebook Researchì˜ GitHub ë¦¬í¬ì§€í† ë¦¬ì— ì ‘ì†í•˜ì…”ì„œ Omnilingual ASRì˜ ì „ì²´ ì†ŒìŠ¤ ì½”ë“œë¥¼ ì‚´í´ë³´ì‹œê³ , ì§ì ‘ ì‹¤í—˜í•´ë³´ì‹œê¸¸ ì¶”ì²œë“œë¦¬ë©´ì„œ ì €ëŠ” ë‹¤ìŒ ì‹œê°„ì— ë” ìœ ìµí•œ ì •ë³´ë¥¼ ê°€ì§€ê³  ë‹¤ì‹œ ì°¾ì•„ëµ™ê² ìŠµë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤.</p>
<p data-ke-size="size16"><b>í•µì‹¬ í‚¤ì›Œë“œ:</b> Omnilingual ASR, ë‹¤êµ­ì–´ ìŒì„± ì¸ì‹, Transformer, Facebook Research, ìë™ ìŒì„± ì¸ì‹, AI ê¸°ìˆ </p>
