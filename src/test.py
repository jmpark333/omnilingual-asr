#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Omnilingual ASR ì‹¤ì „ íŠœí† ë¦¬ì–¼ í†µí•© ì˜ˆì œ
Facebook Researchì˜ Omnilingual ASRì„ í™œìš©í•œ ë‹¤êµ­ì–´ ìŒì„± ì¸ì‹ ë°ëª¨

ì‘ì„±ì: AI ê¸°ìˆ  ë¸”ë¡œê·¸
ë²„ì „: 1.0
"""

import torch
import librosa
import numpy as np
import time
from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor
from jiwer import wer
import os
import warnings

# ê²½ê³  ë©”ì‹œì§€ ë¬´ì‹œ
warnings.filterwarnings("ignore")

class OmnilingualASR:
    """
    Omnilingual ASRì„ ì‰½ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” í´ë˜ìŠ¤
    """
    
    def __init__(self, model_name="facebook/omniASR_CTC_1B"):
        """
        ëª¨ë¸ ì´ˆê¸°í™”
        
        Args:
            model_name (str): ì‚¬ìš©í•  ëª¨ë¸ ì´ë¦„
        """
        print("ğŸš€ Omnilingual ASR ëª¨ë¸ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤...")
        
        # GPU ì‚¬ìš© ì„¤ì •
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"ğŸ“± ì‚¬ìš© ë””ë°”ì´ìŠ¤: {self.device}")
        
        # ëª¨ë¸ê³¼ í”„ë¡œì„¸ì„œ ë¡œë“œ
        print("ğŸ“¥ ëª¨ë¸ê³¼ í”„ë¡œì„¸ì„œë¥¼ ë¡œë“œí•©ë‹ˆë‹¤...")
        self.processor = Wav2Vec2Processor.from_pretrained(model_name)
        self.model = Wav2Vec2ForCTC.from_pretrained(model_name)
        self.model = self.model.to(self.device)
        
        print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
        print(f"ğŸŒ ì§€ì› ì–¸ì–´ ìˆ˜: {self.processor.tokenizer.vocab_size}")
        print(f"ğŸ¯ ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜: {self.model.num_parameters():,}")
        
    def preprocess_audio(self, audio_path, target_sr=16000):
        """
        ìŒì„± íŒŒì¼ì„ ì „ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜
        
        Args:
            audio_path (str): ìŒì„± íŒŒì¼ ê²½ë¡œ
            target_sr (int): ëª©í‘œ ìƒ˜í”Œë§ ë ˆì´íŠ¸
            
        Returns:
            numpy.ndarray: ì „ì²˜ë¦¬ëœ ìŒì„± ë°ì´í„°
        """
        if not os.path.exists(audio_path):
            raise FileNotFoundError(f"âŒ ìŒì„± íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {audio_path}")
        
        # ìŒì„± íŒŒì¼ ë¡œë“œ
        speech, sr = librosa.load(audio_path, sr=target_sr)
        
        # ì •ê·œí™”
        if np.max(np.abs(speech)) > 0:
            speech = speech / np.max(np.abs(speech))
        
        return speech
    
    def transcribe_audio(self, audio_path, language="korean"):
        """
        ë‹¨ì¼ ìŒì„± íŒŒì¼ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
        
        Args:
            audio_path (str): ìŒì„± íŒŒì¼ ê²½ë¡œ
            language (str): ì–¸ì–´ ì„¤ì • (í˜„ì¬ëŠ” ì°¸ê³ ìš©)
            
        Returns:
            str: ë³€í™˜ëœ í…ìŠ¤íŠ¸
        """
        print(f"ğŸµ ìŒì„± íŒŒì¼ ì²˜ë¦¬ ì¤‘: {audio_path}")
        
        # ìŒì„± ì „ì²˜ë¦¬
        speech = self.preprocess_audio(audio_path)
        
        # ì…ë ¥ ê°’ìœ¼ë¡œ ë³€í™˜
        inputs = self.processor(speech, sampling_rate=16000, return_tensors="pt", padding=True)
        inputs = inputs.to(self.device)
        
        # ëª¨ë¸ ì¶”ë¡ 
        with torch.no_grad():
            logits = self.model(**inputs).logits
        
        # í…ìŠ¤íŠ¸ë¡œ ë””ì½”ë”©
        predicted_ids = torch.argmax(logits, dim=-1)
        transcription = self.processor.batch_decode(predicted_ids)
        
        return transcription[0]
    
    def batch_transcribe(self, audio_files, batch_size=4):
        """
        ì—¬ëŸ¬ ìŒì„± íŒŒì¼ì„ ë°°ì¹˜ë¡œ ì²˜ë¦¬
        
        Args:
            audio_files (list): ìŒì„± íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
            batch_size (int): ë°°ì¹˜ í¬ê¸°
            
        Returns:
            list: ë³€í™˜ëœ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
        """
        print(f"ğŸ“¦ ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘ (íŒŒì¼ ìˆ˜: {len(audio_files)}, ë°°ì¹˜ í¬ê¸°: {batch_size})")
        results = []
        
        for i in range(0, len(audio_files), batch_size):
            batch_files = audio_files[i:i+batch_size]
            batch_speeches = []
            
            try:
                # ë°°ì¹˜ ë‚´ ìŒì„± íŒŒì¼ ì „ì²˜ë¦¬
                for audio_file in batch_files:
                    speech = self.preprocess_audio(audio_file)
                    batch_speeches.append(speech)
                
                # ë°°ì¹˜ ì…ë ¥ìœ¼ë¡œ ë³€í™˜
                inputs = self.processor(batch_speeches, sampling_rate=16000, return_tensors="pt", padding=True)
                inputs = inputs.to(self.device)
                
                # ë°°ì¹˜ ì¶”ë¡ 
                with torch.no_grad():
                    logits = self.model(**inputs).logits
                
                # ë°°ì¹˜ ë””ì½”ë”©
                predicted_ids = torch.argmax(logits, dim=-1)
                batch_transcriptions = self.processor.batch_decode(predicted_ids)
                
                results.extend(batch_transcriptions)
                
                print(f"âœ… ë°°ì¹˜ {i//batch_size + 1}/{(len(audio_files)-1)//batch_size + 1} ì™„ë£Œ")
                
            except Exception as e:
                print(f"âŒ ë°°ì¹˜ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                # ì‹¤íŒ¨í•œ íŒŒì¼ë“¤ì€ ë¹ˆ ë¬¸ìì—´ë¡œ ì²˜ë¦¬
                for _ in batch_files:
                    results.append("")
        
        return results
    
    def evaluate_performance(self, transcriptions, ground_truths):
        """
        ìŒì„± ì¸ì‹ ì„±ëŠ¥ì„ í‰ê°€í•˜ëŠ” í•¨ìˆ˜
        
        Args:
            transcriptions (list): ì˜ˆì¸¡ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            ground_truths (list): ì •ë‹µ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            float: í‰ê·  WER (Word Error Rate)
        """
        if len(transcriptions) != len(ground_truths):
            raise ValueError("âŒ ì˜ˆì¸¡ ê²°ê³¼ì™€ ì •ë‹µì˜ ê°œìˆ˜ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        
        total_wer = 0
        print("\nğŸ“Š ì„±ëŠ¥ í‰ê°€ ê²°ê³¼:")
        print("=" * 60)
        
        for i, (pred, truth) in enumerate(zip(transcriptions, ground_truths)):
            current_wer = wer(truth, pred)
            total_wer += current_wer
            
            print(f"ğŸ“ íŒŒì¼ {i+1}:")
            print(f"   ì˜ˆì¸¡: {pred}")
            print(f"   ì •ë‹µ: {truth}")
            print(f"   WER: {current_wer:.4f}")
            print("-" * 40)
        
        avg_wer = total_wer / len(transcriptions)
        print(f"ğŸ¯ í‰ê·  WER: {avg_wer:.4f} ({(1-avg_wer)*100:.2f}% ì •í™•ë„)")
        
        return avg_wer
    
    def create_sample_audio(self, output_path="sample_audio.wav", duration=3, sample_rate=16000):
        """
        í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œ ìŒì„± íŒŒì¼ ìƒì„±
        
        Args:
            output_path (str): ì¶œë ¥ íŒŒì¼ ê²½ë¡œ
            duration (int): ì§€ì†ì‹œê°„ (ì´ˆ)
            sample_rate (int): ìƒ˜í”Œë§ ë ˆì´íŠ¸
        """
        print(f"ğŸ™ï¸ ìƒ˜í”Œ ìŒì„± íŒŒì¼ ìƒì„± ì¤‘: {output_path}")
        
        # ê°„ë‹¨í•œ ì‚¬ì¸íŒŒ ìƒì„± (í…ŒìŠ¤íŠ¸ìš©)
        t = np.linspace(0, duration, int(sample_rate * duration))
        
        # 440Hzì™€ 880Hz ì£¼íŒŒìˆ˜ ì¡°í•© (AéŸ³ç¬¦)
        frequency1 = 440  # A4
        frequency2 = 880  # A5
        
        # ë‘ ì£¼íŒŒìˆ˜ì˜ ì¡°í•©
        audio_data = 0.5 * np.sin(2 * np.pi * frequency1 * t)
        audio_data += 0.3 * np.sin(2 * np.pi * frequency2 * t)
        
        # ì•°í”Œë¦¬íŠœë“œ ì¡°ì ˆ
        audio_data = audio_data * 0.8
        
        # WAV íŒŒì¼ë¡œ ì €ì¥
        import soundfile as sf
        sf.write(output_path, audio_data, sample_rate)
        
        print(f"âœ… ìƒ˜í”Œ ìŒì„± íŒŒì¼ ìƒì„± ì™„ë£Œ: {output_path}")
        return output_path


def main():
    """
    ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
    """
    print("=" * 60)
    print("ğŸ¤– Omnilingual ASR ì‹¤ì „ íŠœí† ë¦¬ì–¼")
    print("=" * 60)
    
    try:
        # 1. ASR ëª¨ë¸ ì´ˆê¸°í™”
        asr = OmnilingualASR()
        
        # 2. ìƒ˜í”Œ ìŒì„± íŒŒì¼ ìƒì„±
        sample_files = []
        for i in range(3):
            sample_path = f"sample_audio_{i+1}.wav"
            asr.create_sample_audio(sample_path, duration=2+i)
            sample_files.append(sample_path)
        
        # 3. ë‹¨ì¼ íŒŒì¼ í…ŒìŠ¤íŠ¸
        print("\nğŸ¯ ë‹¨ì¼ íŒŒì¼ í…ŒìŠ¤íŠ¸:")
        print("-" * 40)
        single_result = asr.transcribe_audio(sample_files[0])
        print(f"ğŸ“ ì¸ì‹ ê²°ê³¼: {single_result}")
        
        # 4. ë°°ì¹˜ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
        print("\nğŸ“¦ ë°°ì¹˜ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸:")
        print("-" * 40)
        start_time = time.time()
        batch_results = asr.batch_transcribe(sample_files, batch_size=2)
        end_time = time.time()
        
        print(f"â±ï¸ ì´ ì²˜ë¦¬ ì‹œê°„: {end_time - start_time:.2f}ì´ˆ")
        print(f"ğŸš€ í‰ê·  ì²˜ë¦¬ ì†ë„: {len(sample_files)/(end_time - start_time):.2f} íŒŒì¼/ì´ˆ")
        
        # 5. ì„±ëŠ¥ í‰ê°€ (ì˜ˆì œ ë°ì´í„°)
        print("\nğŸ“Š ì„±ëŠ¥ í‰ê°€ í…ŒìŠ¤íŠ¸:")
        print("-" * 40)
        
        # ì˜ˆì œ ë°ì´í„° (ì‹¤ì œ ì‚¬ìš© ì‹œì—ëŠ” ì‹¤ì œ ìŒì„± íŒŒì¼ê³¼ ì •ë‹µ í…ìŠ¤íŠ¸ í•„ìš”)
        example_transcriptions = ["hello world", "test recognition", "audio processing"]
        example_ground_truths = ["hello world", "test recognition", "audio processing"]
        
        # ì„±ëŠ¥ í‰ê°€ ì‹¤í–‰
        avg_wer = asr.evaluate_performance(example_transcriptions, example_ground_truths)
        
        # 6. ìµœì¢… ìš”ì•½
        print("\n" + "=" * 60)
        print("ğŸ‰ íŠœí† ë¦¬ì–¼ ì™„ë£Œ!")
        print("=" * 60)
        print("âœ… ì„±ëŠ¥ ì§€í‘œ:")
        print(f"   - í‰ê·  WER: {avg_wer:.4f}")
        print(f"   - ì •í™•ë„: {(1-avg_wer)*100:.2f}%")
        print(f"   - ì²˜ë¦¬ ì†ë„: {len(sample_files)/(end_time - start_time):.2f} íŒŒì¼/ì´ˆ")
        print("\nğŸ’¡ íŒ:")
        print("   - ì‹¤ì œ ìŒì„± íŒŒì¼ì„ ì‚¬ìš©í•˜ë ¤ë©´ sample_audio.wav íŒŒì¼ë“¤ì„ êµì²´í•˜ì„¸ìš”")
        print("   - GPUë¥¼ ì‚¬ìš©í•˜ë©´ ì²˜ë¦¬ ì†ë„ê°€ í¬ê²Œ í–¥ìƒë©ë‹ˆë‹¤")
        print("   - ë” ë‚˜ì€ ì„±ëŠ¥ì„ ìœ„í•´ fine-tuningì„ ê³ ë ¤í•´ë³´ì„¸ìš”")
        print("\nğŸ”— ì°¸ê³ : https://github.com/facebookresearch/omnilingual-asr")
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print("\nğŸ› ï¸ í•´ê²° ë°©ë²•:")
        print("1. ì¸í„°ë„· ì—°ê²° í™•ì¸")
        print("2. í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ í™•ì¸: pip install torch transformers librosa soundfile jiwer")
        print("3. GPU ë“œë¼ì´ë²„ í™•ì¸ (CUDA ì‚¬ìš© ì‹œ)")


if __name__ == "__main__":
    main()
